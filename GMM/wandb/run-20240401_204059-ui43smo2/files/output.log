
Epoch 5: KL Divergence = 7.058413505554199
Epoch 10: KL Divergence = 7.332519054412842
Epoch 15: KL Divergence = 7.770731449127197
Epoch 20: KL Divergence = 8.1858549118042
Epoch 25: KL Divergence = 8.683692932128906
Epoch 30: KL Divergence = 8.829792976379395
Epoch 35: KL Divergence = 9.087401390075684
Epoch 40: KL Divergence = 9.638280868530273
Epoch 45: KL Divergence = 9.817423820495605
Epoch 50: KL Divergence = 10.19705581665039
Epoch 55: KL Divergence = 9.981371879577637
Epoch 60: KL Divergence = 9.903189659118652
Epoch 65: KL Divergence = 10.539299011230469
Epoch 70: KL Divergence = 10.782722473144531
Epoch 75: KL Divergence = 10.603267669677734
Epoch 80: KL Divergence = 10.586162567138672
Epoch 85: KL Divergence = 10.84894847869873
Epoch 90: KL Divergence = 10.540553092956543
Epoch 95: KL Divergence = 10.34880256652832
Traceback (most recent call last):
  File "/home/luyao/amortized_vi_trpl/GMM/GMM_model.py", line 253, in <module>
    plot2d_matplotlib(target, model, contexts)
  File "/home/luyao/amortized_vi_trpl/GMM/GMM_plot.py", line 41, in plot2d_matplotlib
    max_y=max_y,
  File "/home/luyao/amortized_vi_trpl/GMM/GMM_plot.py", line 131, in compute_data_for_plot2d
    relevant_means[:, 0].min(),
RuntimeError: min(): Expected reduction dim to be specified for input.numel() == 0. Specify the reduction dim with the 'dim' argument.
Epoch 100: KL Divergence = 10.298216819763184
Training done!
contexts: tensor([[1.1076],
        [0.0448],
        [0.0663]])
target mean: tensor([[[8.9463, 4.4680]],
        [[0.4482, 9.9900]],
        [[0.6624, 9.9780]]])
weight here [[0.]
 [0.]
 [0.]]