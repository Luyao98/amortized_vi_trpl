
Epoch 5: KL Divergence = 6.136085510253906
Epoch 10: KL Divergence = 6.932744979858398
Training done!
contexts: tensor([[-1.6594],
        [ 2.0980],
        [-1.5777]])
target mean: tensor([[[-9.9608, -0.8845]],
        [[ 8.6421, -5.0314]],
        [[-9.9998, -0.0692]]])
weight here [[1.]
 [1.]
 [1.]]
model mean: [[[-2.8504038 -2.1475015]]
 [[-3.3448448 -2.5401733]]
 [[-2.8302517 -2.137832 ]]]