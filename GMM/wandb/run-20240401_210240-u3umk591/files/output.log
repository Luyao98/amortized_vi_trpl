
Epoch 5: KL Divergence = 7.318506717681885
Epoch 10: KL Divergence = 7.311392307281494
Epoch 15: KL Divergence = 7.7393927574157715
Epoch 20: KL Divergence = 7.923870086669922
Epoch 25: KL Divergence = 8.532634735107422
Epoch 30: KL Divergence = 9.176704406738281
Epoch 35: KL Divergence = 9.4940767288208
Epoch 40: KL Divergence = 9.400029182434082
Epoch 45: KL Divergence = 9.872964859008789
Epoch 50: KL Divergence = 10.506043434143066
Epoch 55: KL Divergence = 10.724034309387207
Epoch 60: KL Divergence = 10.91312026977539
Epoch 65: KL Divergence = 11.00302505493164
Epoch 70: KL Divergence = 10.71401596069336
Epoch 75: KL Divergence = 10.741351127624512
Epoch 80: KL Divergence = 10.465779304504395
Epoch 85: KL Divergence = 10.572478294372559
Epoch 90: KL Divergence = 10.439118385314941
Epoch 95: KL Divergence = 10.495980262756348
Epoch 100: KL Divergence = 10.38541030883789
Training done!
contexts: tensor([[ 1.3447],
        [-0.9326],
        [ 0.7034]])
target mean: tensor([[[ 9.7454,  2.2420]],
        [[-8.0319,  5.9573]],
        [[ 6.4683,  7.6263]]])
weight here [[1.]
 [1.]
 [1.]]
model mean: [[[-2.5874496   0.00994073]]
 [[-2.5830534   0.00522842]]
 [[-2.5855727   0.00851531]]]