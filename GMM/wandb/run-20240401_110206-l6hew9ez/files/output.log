
Epoch 5: KL Divergence = 6.756218433380127
Epoch 10: KL Divergence = 6.939181804656982
Training done!
contexts: tensor([[-0.8168],
        [ 1.0903],
        [-0.1389]])
target mean: tensor([[[-7.2897,  6.8454]],
        [[ 8.8678,  4.6219]],
        [[-1.3845,  9.9037]]])
weight here [[1.]
 [1.]
 [1.]]
model mean: [[[-3.4872808 -2.2111723]]
 [[-3.5207298 -2.2562535]]
