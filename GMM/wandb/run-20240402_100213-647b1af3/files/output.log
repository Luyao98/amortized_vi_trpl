
Epoch 5: KL Divergence = 6.98501443862915
Epoch 10: KL Divergence = 7.122941493988037
Training done!
contexts: tensor([[2.1037],
        [0.8652],
        [2.3536]])
target mean: tensor([[[ 8.6132, -5.0806]],
        [[ 7.6122,  6.4849]],
        [[ 7.0894, -7.0527]]])
weight here [[1.]
 [1.]
 [1.]]
model mean: [[[-2.6259890e+00 -2.1494711e-03]]
 [[-2.6833119e+00  3.0501477e-02]]
 [[-2.6364799e+00  5.4221507e-04]]]