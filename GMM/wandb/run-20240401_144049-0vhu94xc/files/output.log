
Epoch 5: KL Divergence = 6.095594882965088
Epoch 10: KL Divergence = 6.548905849456787
Training done!
contexts: tensor([[-1.0981],
        [ 0.8808],
        [ 1.2236]])
target mean: tensor([[[-8.9036,  4.5525]],
        [[ 7.7127,  6.3650]],
        [[ 9.4034,  3.4022]]])
weight here [[1.]
 [1.]
 [1.]]
model mean: [[[-3.1160824 -2.2437012]]
 [[-2.9510248 -2.105134 ]]
 [[-2.8700523 -2.0424771]]]