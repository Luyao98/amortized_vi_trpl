
Epoch 5: KL Divergence = 6.4465131759643555
Epoch 10: KL Divergence = 6.5707502365112305
Training done!
contexts: tensor([[-1.0418],
        [ 1.6911],
        [-0.3275]])
target mean: tensor([[[-8.6334,  5.0463]],
        [[ 9.9278, -1.1998]],
        [[-3.2169,  9.4684]]])
weight here [[1.]
 [1.]
 [1.]]
model mean: [[[-3.3780708 -2.2569766]]
 [[-3.1771789 -2.2522376]]
 [[-3.124068  -2.034436 ]]]