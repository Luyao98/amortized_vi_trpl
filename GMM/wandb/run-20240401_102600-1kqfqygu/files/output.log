
Epoch 5: KL Divergence = 7.066655158996582
Epoch 10: KL Divergence = 7.235950946807861
Training done!
contexts: tensor([[0.2940],
        [0.0156],
        [2.1448]])
target mean: tensor([[[ 2.8977,  9.5710]],
        [[ 0.1563,  9.9988]],
        [[ 8.3975, -5.4297]]])
weight here [[1.]
 [1.]
 [1.]]
model mean: [[[-2.4051356 -1.8822521]]
 [[-2.3464532 -1.8441896]]
 [[-3.1141937 -2.4751098]]]