Traceback (most recent call last):
  File "/home/luyao/amortized_vi_trpl/GMM/GMM_model.py", line 163, in <module>
    train_model(model, target, n_epochs, batch_size, n_context, n_components, eps, beta, alpha, optimizer, device)
  File "/home/luyao/amortized_vi_trpl/GMM/GMM_model.py", line 110, in train_model
    loss.backward(retain_graph=True)
  File "/home/luyao/miniconda3/envs/ITPAL_luyao/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/luyao/miniconda3/envs/ITPAL_luyao/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [64, 3]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).