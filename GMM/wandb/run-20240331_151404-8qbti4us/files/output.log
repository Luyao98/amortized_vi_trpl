
Epoch 5: KL Divergence = 7.253328800201416
Epoch 10: KL Divergence = 7.585625171661377
Epoch 15: KL Divergence = 7.483001232147217
Epoch 20: KL Divergence = 7.853245735168457
Epoch 25: KL Divergence = 8.103995323181152
Epoch 30: KL Divergence = 8.171707153320312
Epoch 35: KL Divergence = 8.439300537109375
Epoch 40: KL Divergence = 8.474150657653809
Epoch 45: KL Divergence = 8.566250801086426
Epoch 50: KL Divergence = 8.439638137817383
Epoch 55: KL Divergence = 8.25073528289795
Epoch 60: KL Divergence = 8.65569019317627
Epoch 65: KL Divergence = 8.483709335327148
Epoch 70: KL Divergence = 8.8285551071167
Epoch 75: KL Divergence = 8.536659240722656
Epoch 80: KL Divergence = 8.598518371582031
Epoch 85: KL Divergence = 8.554683685302734
Epoch 90: KL Divergence = 8.45095157623291
Epoch 95: KL Divergence = 8.450113296508789
Epoch 100: KL Divergence = 8.463886260986328
Training done!
contexts: tensor([[-0.0785],
        [ 0.1052],
        [ 0.5380]])
target mean: tensor([[[-0.7845,  9.9692]],
        [[ 1.0501,  9.9447]],
        [[ 5.1244,  8.5872]]])
weight here [[1.]
 [1.]
 [1.]]
model mean: [[[-2.9303715 -2.4010577]]
 [[-2.9303715 -2.4010577]]
 [[-2.9303715 -2.4010577]]]