
Epoch 5: KL Divergence = 7.046234607696533
Epoch 10: KL Divergence = 7.131501197814941
Epoch 15: KL Divergence = 7.372407913208008
Epoch 20: KL Divergence = 7.698801040649414
Epoch 25: KL Divergence = 8.002067565917969
Epoch 30: KL Divergence = 8.53792667388916
Epoch 35: KL Divergence = 8.695534706115723
Epoch 40: KL Divergence = 8.973782539367676
Epoch 45: KL Divergence = 9.275257110595703
Epoch 50: KL Divergence = 9.305556297302246
Epoch 55: KL Divergence = 9.426302909851074
Epoch 60: KL Divergence = 9.437443733215332
Epoch 65: KL Divergence = 9.682076454162598
Epoch 70: KL Divergence = 9.856654167175293
Epoch 75: KL Divergence = 9.788458824157715
Epoch 80: KL Divergence = 9.716348648071289
Epoch 85: KL Divergence = 9.821843147277832
Epoch 90: KL Divergence = 9.935924530029297
Epoch 95: KL Divergence = 9.85400104522705
Epoch 100: KL Divergence = 9.997199058532715
Training done!
contexts: tensor([[-2.4341],
        [ 1.5037],
        [-1.4946]])
target mean: tensor([[[-6.4992, -7.6001]],
        [[ 9.9775,  0.6709]],
        [[-9.9710,  0.7616]]])
weight here [[1.]
 [1.]
 [1.]]
model mean: [[[-3.1518826 -2.3873394]]
 [[-3.193779  -2.4173791]]
 [[-3.1573758 -2.393062 ]]]