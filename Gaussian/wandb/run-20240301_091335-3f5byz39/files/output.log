/home/luyao/amortized_vi_trpl/2DGaussian/model.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  contexts = torch.tensor(contexts, dtype=torch.float32).to(device)
Traceback (most recent call last):
  File "/home/luyao/amortized_vi_trpl/2DGaussian/model.py", line 190, in <module>
    loss.backward()
  File "/home/luyao/miniconda3/envs/ITPAL_luyao/lib/python3.7/site-packages/torch/_tensor.py", line 489, in backward
    self, gradient, retain_graph, create_graph, inputs=inputs
  File "/home/luyao/miniconda3/envs/ITPAL_luyao/lib/python3.7/site-packages/torch/autograd/__init__.py", line 199, in backward
    allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn
tensor([[-1.8528, -0.8333],
        [-1.3491,  2.7668],
        [-2.4680, -2.9609],
        ...,
        [ 1.6642, -2.3895],
        [ 1.4833, -1.6009],
        [ 2.4666, -1.5299]])